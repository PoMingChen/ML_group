---
title: "ML_group_presentation1"
author: "PoMingChen"
date: "2018/10/21"
output: html_document
---

### Loss function

> you have separate the continuous outcome and categorical outcome

```{r build a square erro function}
# %*% 是矩陣相乘的意思

sqerrloss = function(beta, X, y){
  mu = X %*% beta
  sum((y-mu)^2)
}
```


```{r}
# data setup
set.seed(123)                              # for reproducibility
N = 100                                    # sample size
X = cbind(1, X1=rnorm(N), X2=rnorm(N))     # model matrix: intercept, 2 predictors #rnorm, to build a                                                normal distribution with mean = 0, sd =1
beta = c(0, -.5, .5)                       # true coef values. we give them (0,-0.5,0.5) as default
y = rnorm(N, X%*%beta, sd=1)               # target
y
```

```{r}
# results
#optim, General-purpose optimization
our_func = optim(par=c(0,0,0),             # starting values
                 fn=sqerrloss, 
                 X=X, 
                 y=y, 
                 method='BFGS')

#arguments par, Initial values for the parameters to be optimized over.

#Method "BFGS" is a quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.

data.frame(X[,-1])

# our_func
lm_result = lm(y ~ ., data.frame(X[,-1]))  # check with lm 

rbind(optim=c(our_func$par, our_func$value), 
      lm=c(coef(lm_result), loss=sum(resid(lm_result)^2)))
```

### regularization

```{r}
set.seed(123)
N = 100
X = cbind(1, matrix(rnorm(N*10), ncol=10))
X
beta = runif(ncol(X))
#runif generates random deviates
y =  rnorm(N, X%*%beta, sd=2)

y
```

```{r}
sqerrloss_reg = function(beta, X, y, lambda=.5){
  mu = X%*%beta
  # sum((y-mu)^2) + lambda*sum(abs(beta[-1])) # conceptual
  sum((y-mu)^2) + 2*length(y)*lambda*sum(abs(beta[-1])) # actual for lasso
}

#beta[-1]，就是那些coeffient，然後要取abs()絕對值。
length(y)

lm_result = lm(y~., data.frame(X[,-1]) )
regularized_result = optim(par=rep(0, ncol(X)), 
                           fn=sqerrloss_reg, 
                           X=X, 
                           y=y, 
                           method='BFGS')
```

#### test in regularization

```{r}
# Create test data
N_test = 50
X_test = cbind(1, matrix(rnorm(N_test*10), ncol=10))
y_test = rnorm(N_test, X_test%*%beta, sd=2)
```

```{r}
# fits on training set
fits_lm = fitted(lm_result, newdata = data.frame(X))
fits_reg = X%*%regularized_result$par


# loss on training set
data.frame(lm_train = crossprod(y - fits_lm),
           regularized_train = crossprod(y - fits_reg))
```

```{r}
# fits on test set
fits_lm = predict(lm_result, newdata = data.frame(X_test))
fits_reg = X_test%*%regularized_result$par

# loss on test set，我的理解是，有下降是真的有變好？
data.frame(lm_test = crossprod(y_test - fits_lm),
           regularized_test = crossprod(y_test - fits_reg))
```


